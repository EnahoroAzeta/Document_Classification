{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b861fdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f14c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/Documents/Kaggle/TextClassification'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b078cd09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/apple/Documents/Kaggle/TextClassification/bbc-text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0b8ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   text      2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5377f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"]=data[\"category\"]\n",
    "data[\"input\"]= data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928a45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"category\",\"text\"], axis =1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21aa5cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'business', 'sport', 'entertainment', 'politics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011acd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"].replace({\"tech\": 0, \"business\":1, \"sport\":2, \"entertainment\":3, \"politics\":4}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78020be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vector for text\n",
    "count_vect = CountVectorizer(analyzer = \"word\")\n",
    "count_vec_X = count_vect.fit_transform(data[\"input\"])\n",
    "\n",
    "cvtrain_x,cvtest_x,cvtrain_y,cvtest_y = train_test_split(count_vec_X,data[\"label\"],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab30c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1780x29421 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 358755 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvtrain_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "628cb304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1780, 29421)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvtrain_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2935a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vector for text on word level\n",
    "tfidf_obj = TfidfVectorizer(analyzer = \"word\", max_features = 5000)\n",
    "tfidf_vec_X = tfidf_obj.fit_transform(data[\"input\"])\n",
    "tfidf_train_x,tfidf_test_x,tfidf_train_y,tfidf_test_y = train_test_split(tfidf_vec_X,data[\"label\"],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "150684c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf on ngram level\n",
    "tfidf_ngram = TfidfVectorizer(analyzer = \"word\", ngram_range =(2,3),max_features = 5000)\n",
    "tfidf_ngram_X = tfidf_ngram.fit_transform(data[\"input\"])\n",
    "ngram_train_x,ngram_test_x,ngram_train_y,ngram_test_y = train_test_split(tfidf_vec_X,data[\"label\"],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function for model building\n",
    "def train_model(model_classifier, train_x,test_x, train_y,test_y):\n",
    "    model_classifier.fit(train_x,train_y)\n",
    "    \n",
    "    prediction = model_classifier.predict(test_x)\n",
    "    \n",
    "    print(prediction)\n",
    "    \n",
    "    return metrics.accuracy_score(prediction, test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44083d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d06e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 1 4 4 3 1 4 3 3 3 0 2 2 2 3 2 1 2 1 0 0 0 2 2 2 3 4 1 0 4 1 4 2 2\n",
      " 0 2 1 4 4 0 1 3 1 2 0 1 2 3 4 3 0 2 1 0 2 4 3 3 3 0 4 3 1 4 4 2 0 1 4 1 1\n",
      " 0 2 1 0 1 0 4 2 2 1 0 1 3 2 4 3 2 0 2 1 0 1 0 4 1 4 1 3 1 4 1 0 4 0 1 3 2\n",
      " 3 1 2 0 4 2 3 4 3 4 1 0 0 3 2 4 0 3 4 3 1 3 4 2 2 2 4 4 4 1 1 0 4 1 4 4 1\n",
      " 1 3 1 2 0 4 1 2 3 1 1 4 2 0 2 4 3 2 1 0 0 1 3 3 2 2 3 4 2 3 3 1 1 2 3 4 0\n",
      " 1 4 4 3 3 4 2 3 1 3 1 1 0 4 3 4 2 1 3 4 3 0 3 1 2 0 3 0 3 2 0 0 0 4 2 3 3\n",
      " 3 2 2 1 2 3 2 4 2 3 1 2 0 1 0 2 2 2 4 4 2 2 2 3 4 1 2 4 1 1 0 2 3 0 1 3 1\n",
      " 2 2 4 1 4 4 3 3 3 1 2 0 3 0 4 2 0 4 4 0 1 1 3 0 3 1 4 0 3 4 0 0 2 3 3 4 4\n",
      " 3 4 2 1 3 2 2 0 0 0 1 0 1 4 2 2 4 4 1 0 2 2 2 4 1 4 0 0 4 2 0 0 2 2 4 1 1\n",
      " 2 0 0 0 0 3 1 1 0 1 3 3 0 2 2 1 1 1 2 1 1 1 2 4 3 2 4 3 4 4 0 2 4 4 4 1 3\n",
      " 1 4 3 3 4 2 3 1 1 0 2 1 0 2 1 2 2 2 4 3 3 3 1 1 1 2 1 0 0 3 4 1 1 2 2 4 0\n",
      " 0 1 2 3 1 2 1 2 1 4 1 2 2 4 4 1 1 4 2 0 0 1 1 3 3 1 3 2 4 1 4 0 1 3 4 4 1\n",
      " 4]\n",
      "NB, Count Vectors:  0.9865168539325843\n",
      "[2 1 1 3 4 1 2 0 1 3 3 2 1 3 3 0 3 2 2 1 4 4 1 3 2 1 1 4 3 4 2 1 3 4 3 1 4\n",
      " 1 0 2 1 1 4 3 1 1 0 2 1 1 4 2 3 2 0 1 4 0 0 1 2 4 0 2 1 4 1 3 1 2 1 2 0 1\n",
      " 0 3 1 2 4 4 4 0 4 1 2 2 2 0 3 1 3 4 1 2 0 2 3 1 0 4 0 4 4 2 4 1 1 0 2 3 4\n",
      " 2 3 3 1 1 0 3 1 4 1 2 0 4 0 1 4 4 0 0 1 1 3 4 0 1 2 0 2 3 1 1 0 2 0 4 1 3\n",
      " 3 3 0 4 2 3 1 3 3 0 4 4 2 0 3 1 1 4 2 4 4 2 1 3 3 0 0 4 2 1 3 1 0 4 4 2 4\n",
      " 4 2 2 4 1 2 3 1 1 0 2 2 2 2 1 1 4 4 1 2 2 1 4 1 3 2 0 0 2 3 0 1 2 4 3 2 2\n",
      " 1 4 2 4 3 2 1 2 1 0 4 0 0 2 1 0 3 0 4 0 1 0 4 1 0 1 0 0 3 4 0 4 2 2 1 4 2\n",
      " 2 0 0 0 2 2 2 2 1 2 1 1 0 4 4 3 0 1 4 2 2 0 1 0 0 0 1 1 2 4 1 2 3 2 4 3 0\n",
      " 3 2 2 3 1 0 0 1 1 1 2 1 2 0 1 1 4 1 4 4 4 1 0 4 4 4 2 4 0 2 4 2 4 0 4 4 3\n",
      " 4 1 0 3 1 2 4 2 1 2 4 1 2 3 1 2 0 3 2 3 1 2 2 3 1 0 1 2 1 1 2 4 0 2 0 4 4\n",
      " 1 1 1 2 4 2 2 3 3 2 0 2 1 2 4 2 3 2 3 2 1 2 2 1 3 2 4 2 2 1 4 1 3 4 4 3 0\n",
      " 1 3 0 3 1 4 3 2 1 1 3 1 4 3 1 0 1 3 2 3 0 1 4 0 2 4 3 2 3 2 0 4 3 2 0 3 4\n",
      " 0]\n",
      "NB,Tfidf word level Vectors:  0.9707865168539326\n",
      "[0 2 0 2 0 0 4 4 4 1 2 1 3 0 1 4 4 0 3 4 3 1 2 4 1 1 2 4 4 4 0 2 3 0 0 2 0\n",
      " 3 2 0 3 3 4 1 1 2 1 1 1 1 2 0 0 0 0 3 3 1 4 0 4 1 3 1 2 4 0 2 1 3 4 4 3 4\n",
      " 3 0 3 3 0 1 0 4 2 2 2 0 2 3 2 3 1 3 3 3 2 0 1 4 4 4 1 2 1 0 4 1 2 1 1 0 0\n",
      " 1 2 1 2 2 4 3 2 1 2 3 4 3 0 4 4 0 3 4 2 1 0 2 2 1 2 0 3 3 1 3 1 1 2 4 4 3\n",
      " 2 2 4 1 3 0 2 4 0 1 2 4 0 0 2 4 0 2 3 2 2 3 1 1 2 0 2 1 4 3 1 2 0 3 2 1 1\n",
      " 4 1 4 4 3 0 4 4 3 1 4 3 4 4 0 2 1 0 2 4 4 1 2 4 0 3 0 4 1 2 2 1 2 3 3 2 1\n",
      " 0 1 1 0 4 2 0 4 2 3 0 2 1 0 0 0 4 1 4 3 3 2 4 3 4 4 2 1 0 1 3 2 3 1 3 2 2\n",
      " 1 4 1 0 1 2 0 1 4 0 2 4 4 4 1 1 4 4 0 3 3 1 1 1 0 3 0 1 0 0 1 0 4 3 3 3 0\n",
      " 3 2 2 2 3 2 1 4 4 3 2 3 1 2 2 1 0 2 3 0 3 0 2 1 4 3 0 4 1 3 2 4 2 3 1 2 1\n",
      " 2 1 3 4 2 3 4 2 1 0 1 1 3 0 2 3 3 2 2 2 2 3 2 0 1 4 0 3 4 0 2 3 2 2 0 4 3\n",
      " 4 4 0 4 3 1 0 0 3 3 2 0 1 3 0 0 2 1 2 1 1 4 4 4 3 2 2 0 2 3 2 3 3 2 3 0 2\n",
      " 1 3 2 0 4 4 2 3 1 0 0 2 4 3 0 1 0 4 2 2 2 1 4 0 1 2 2 0 2 1 0 0 0 3 1 3 4\n",
      " 1]\n",
      "NB,Tfidf ngram level Vectors:  0.9595505617977528\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "# naivebayes model on count vector\n",
    "accuracy_naiveBayes = train_model(naive_bayes.MultinomialNB(),cvtrain_x,cvtest_x,cvtrain_y,cvtest_y)\n",
    "print(\"NB, Count Vectors: \", accuracy_naiveBayes)\n",
    "\n",
    "# naivebayes model on tfidf vector\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(),tfidf_train_x,tfidf_test_x,tfidf_train_y,tfidf_test_y)\n",
    "print(\"NB,Tfidf word level Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "# naivebayes model on tfidf ngram vector\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(),ngram_train_x,ngram_test_x,ngram_train_y,ngram_test_y)\n",
    "print(\"NB,Tfidf ngram level Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0c516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c376ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.16/libexec/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 1 4 4 3 1 4 3 3 3 0 2 2 2 3 2 1 2 1 0 0 0 2 2 2 3 4 1 0 4 1 4 2 2\n",
      " 0 2 1 4 4 0 1 3 1 2 1 1 2 3 4 3 0 2 1 0 2 4 3 3 3 0 4 3 1 4 4 2 0 1 4 1 3\n",
      " 0 2 1 0 1 0 4 2 2 1 0 1 3 2 4 3 2 0 2 1 0 1 0 4 1 4 1 3 1 4 1 0 4 0 1 3 2\n",
      " 3 1 2 0 4 2 3 4 3 4 1 0 0 3 2 4 0 3 4 3 1 3 4 2 2 2 4 4 4 1 1 0 4 1 4 4 1\n",
      " 1 3 1 2 0 4 1 2 3 1 1 4 2 0 2 4 3 2 1 0 0 1 3 3 2 2 3 4 2 3 3 1 1 2 3 0 0\n",
      " 1 4 4 3 3 4 2 3 1 3 1 1 0 4 3 4 3 1 3 4 3 0 3 1 2 0 3 0 3 2 0 0 0 4 2 3 3\n",
      " 3 2 2 1 2 3 2 4 2 3 1 2 0 1 0 2 2 2 4 4 2 2 2 3 4 1 2 4 1 1 0 2 3 0 1 3 1\n",
      " 2 2 4 1 4 4 3 3 3 1 2 0 3 0 4 2 0 4 4 0 1 1 3 0 3 1 4 0 3 4 0 0 2 3 3 4 4\n",
      " 3 4 2 1 3 2 2 0 0 0 1 1 1 4 2 2 4 4 1 0 2 2 2 4 1 4 0 0 4 2 0 1 2 2 4 1 1\n",
      " 2 0 0 0 0 3 1 1 0 1 3 3 0 2 2 1 1 1 2 1 1 1 2 4 3 2 4 3 4 4 0 2 4 4 4 1 3\n",
      " 1 4 3 3 4 2 3 1 1 0 2 1 0 2 1 2 2 2 4 3 3 3 1 1 1 2 1 0 0 3 2 1 1 2 2 4 0\n",
      " 0 1 2 3 1 2 1 2 1 4 1 2 2 0 4 1 1 4 2 0 0 1 1 3 3 1 4 2 4 1 4 0 1 3 4 4 1\n",
      " 4]\n",
      "Logistic regression,count Vectors:  0.9842696629213483\n",
      "[2 1 1 3 4 1 2 0 1 3 3 2 1 3 3 0 3 2 2 1 4 4 1 3 2 1 1 4 3 4 2 1 3 4 3 1 4\n",
      " 1 0 2 1 1 4 3 1 1 0 2 1 1 4 2 3 2 0 1 4 0 0 1 2 4 0 2 1 4 1 3 1 2 1 2 0 1\n",
      " 0 3 1 2 4 4 4 0 4 1 2 2 2 0 3 1 3 4 1 2 0 2 3 1 0 4 0 4 3 2 4 1 1 0 2 3 4\n",
      " 2 1 3 1 1 0 3 1 4 1 2 0 4 0 1 4 4 0 0 1 1 3 4 0 1 2 0 2 3 1 1 0 2 0 4 1 3\n",
      " 3 3 0 4 2 3 1 3 3 0 4 4 2 0 3 1 1 4 2 4 4 2 1 3 3 0 0 4 2 1 3 1 0 4 4 2 4\n",
      " 3 2 2 4 1 2 3 1 1 0 2 2 2 2 1 1 4 4 1 2 2 1 4 1 3 2 0 0 2 3 0 1 2 4 3 2 2\n",
      " 1 4 2 4 3 2 1 2 1 0 4 0 0 2 1 0 3 0 4 0 1 0 4 1 0 1 0 0 3 4 0 4 2 2 1 4 2\n",
      " 0 0 0 0 2 2 2 2 1 2 1 1 0 4 4 3 0 1 4 2 2 0 1 0 0 0 1 1 2 4 1 2 3 2 4 3 0\n",
      " 3 2 2 3 1 0 0 1 1 1 2 1 2 0 1 1 4 1 4 4 4 1 0 4 4 4 2 4 0 2 4 2 4 0 4 4 3\n",
      " 4 1 0 3 1 2 4 2 1 2 4 1 2 3 1 2 0 3 2 3 1 2 2 3 1 0 1 2 1 1 2 4 0 2 0 4 4\n",
      " 1 1 1 2 4 2 2 3 3 2 0 0 1 2 4 2 3 2 3 2 1 2 2 1 3 2 4 2 2 1 4 1 3 4 4 3 0\n",
      " 1 3 0 3 1 4 3 2 1 1 3 1 4 3 1 0 1 3 2 3 0 1 4 0 2 4 3 2 3 2 0 4 3 2 0 3 4\n",
      " 0]\n",
      "Logistic Regression,Tfidf word level Vectors:  0.9820224719101124\n",
      "[0 2 0 2 0 0 4 4 4 1 2 1 3 0 1 4 4 0 3 4 3 1 2 4 1 1 2 4 4 4 0 2 3 0 0 2 0\n",
      " 3 2 0 3 3 3 1 1 2 1 0 1 1 2 0 0 0 0 3 3 1 4 0 4 1 3 1 2 4 0 2 1 3 4 4 3 4\n",
      " 3 0 3 3 0 1 0 4 2 2 2 0 2 3 2 3 1 3 3 3 2 0 1 4 4 4 1 2 1 0 4 1 2 1 1 0 0\n",
      " 1 2 1 2 2 4 3 2 1 2 3 2 3 0 4 4 0 3 0 2 1 0 2 0 1 2 0 3 3 1 3 1 1 2 4 4 3\n",
      " 2 2 4 1 3 0 2 4 0 1 2 4 0 0 2 4 0 2 3 2 2 3 4 1 2 0 2 1 4 3 1 2 0 3 2 1 1\n",
      " 4 1 4 4 3 0 4 4 3 1 4 3 4 4 0 2 1 0 2 4 4 1 2 4 0 3 0 4 1 2 2 1 2 3 3 2 1\n",
      " 0 1 1 0 4 2 0 4 2 3 0 2 1 0 0 0 4 1 4 3 3 2 4 3 4 4 2 1 0 1 3 2 3 1 3 2 2\n",
      " 1 4 1 0 1 2 0 1 4 0 2 4 4 4 1 1 0 4 0 3 3 1 1 1 0 3 0 1 0 0 1 0 4 3 3 3 0\n",
      " 3 2 2 2 3 2 1 4 4 3 2 3 1 2 2 1 0 2 3 0 3 0 2 1 4 3 0 4 1 3 2 4 2 3 1 2 1\n",
      " 2 1 3 4 2 3 4 2 1 1 1 1 3 0 2 3 3 2 2 2 2 3 2 0 1 4 0 3 4 0 2 3 2 2 0 4 3\n",
      " 4 4 0 4 3 1 0 0 3 3 2 0 1 3 0 0 2 1 2 1 1 4 4 0 3 2 2 0 2 3 2 3 3 2 3 0 2\n",
      " 1 3 2 0 4 4 2 2 1 0 0 2 4 3 0 1 0 4 2 2 2 1 4 0 1 2 2 0 2 1 0 0 0 3 1 3 4\n",
      " 1]\n",
      "Logistic Regression,Tfidf ngram Vectors:  0.9707865168539326\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(),cvtrain_x,cvtest_x,cvtrain_y,cvtest_y)\n",
    "print(\"Logistic regression,count Vectors: \", accuracy)\n",
    "\n",
    "# Logistic Regression model on tfidf vector\n",
    "accuracy = train_model(linear_model.LogisticRegression(),tfidf_train_x,tfidf_test_x,tfidf_train_y,tfidf_test_y)\n",
    "print(\"Logistic Regression,Tfidf word level Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "# Logistic Regression model on tfidf ngram vector\n",
    "accuracy = train_model(linear_model.LogisticRegression(),ngram_train_x,ngram_test_x,ngram_train_y,ngram_test_y)\n",
    "print(\"Logistic Regression,Tfidf ngram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e409cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 1 4 4 3 1 4 3 3 3 0 2 2 2 3 2 1 2 1 0 0 0 2 2 2 3 4 1 0 4 1 4 2 2\n",
      " 0 2 1 2 4 0 1 3 1 2 1 1 2 3 4 3 0 2 1 0 2 4 3 3 3 0 4 3 1 4 4 2 0 1 4 1 3\n",
      " 0 2 1 0 1 0 4 2 2 1 0 1 3 2 4 3 2 0 2 1 0 1 0 0 1 4 1 3 1 2 1 0 4 0 1 3 2\n",
      " 3 1 2 0 4 2 3 4 3 4 1 0 0 3 2 4 0 3 4 3 1 3 4 2 2 2 4 4 4 1 1 0 4 1 4 2 1\n",
      " 1 3 1 2 0 4 4 2 3 1 1 4 2 0 2 4 3 2 1 1 0 1 3 3 2 2 3 4 2 3 3 1 1 2 3 0 0\n",
      " 1 4 4 3 3 4 2 3 1 3 1 1 0 4 3 4 3 1 3 4 2 0 3 1 2 0 3 0 3 2 0 0 0 2 2 3 3\n",
      " 3 2 2 1 2 3 2 4 2 3 1 2 1 1 0 2 2 2 4 4 2 2 2 3 4 1 2 4 1 1 0 2 3 0 1 3 1\n",
      " 2 2 4 1 4 4 3 3 3 1 2 0 3 0 4 2 0 4 4 0 1 1 3 0 3 1 4 0 3 4 0 0 2 3 3 4 4\n",
      " 3 4 2 1 3 2 2 0 0 0 1 0 1 4 2 2 1 4 1 0 2 2 2 4 1 4 0 0 4 2 0 0 2 2 4 1 1\n",
      " 2 0 0 0 0 3 1 1 0 3 3 3 0 2 2 1 1 1 2 1 1 1 2 4 3 2 4 3 4 4 0 2 4 4 4 1 3\n",
      " 1 4 3 3 4 2 3 1 1 0 2 1 0 2 4 2 2 2 4 3 2 3 1 1 1 2 1 0 0 3 2 1 1 2 2 4 0\n",
      " 0 1 2 3 1 2 1 2 1 4 1 2 2 4 4 1 1 4 2 1 0 1 1 3 1 1 0 2 4 1 4 0 1 3 4 4 1\n",
      " 4]\n",
      "Support vector machine,count Vectors:  0.952808988764045\n",
      "[2 1 1 3 4 1 2 0 1 3 3 2 1 3 3 0 3 2 2 1 4 4 1 3 2 1 1 4 3 4 2 1 3 4 3 1 4\n",
      " 1 0 2 1 1 4 3 1 1 0 2 1 1 4 2 3 2 0 1 4 0 0 1 2 4 0 2 1 4 1 3 1 2 1 2 0 1\n",
      " 0 3 1 2 4 4 4 0 4 1 2 2 2 0 3 1 3 4 1 2 0 2 3 1 0 4 0 4 3 2 4 1 1 0 2 3 4\n",
      " 2 3 3 1 1 0 3 1 4 1 2 0 4 0 1 4 4 0 0 1 1 3 4 0 1 2 0 2 3 1 1 0 2 0 4 1 3\n",
      " 3 3 0 4 2 3 1 3 3 0 4 4 2 0 3 1 1 4 2 4 4 2 1 3 3 0 0 4 2 1 3 1 0 4 4 2 4\n",
      " 3 2 2 4 1 2 3 1 1 0 2 2 2 2 1 1 4 4 1 2 2 1 4 1 3 2 0 0 2 3 0 1 2 4 3 2 2\n",
      " 1 4 2 4 3 2 1 2 1 0 4 0 0 2 1 0 3 0 4 0 1 0 4 1 0 1 0 0 3 4 0 4 2 2 1 4 2\n",
      " 0 0 0 0 2 2 2 2 1 2 1 1 0 4 4 3 0 1 4 2 2 0 1 0 0 0 1 1 2 4 1 2 3 2 4 3 0\n",
      " 3 2 2 3 1 0 0 1 1 1 2 1 2 0 1 1 4 1 4 4 4 1 3 4 4 4 2 4 0 2 4 2 4 0 4 4 3\n",
      " 4 1 0 3 1 2 4 2 1 2 4 1 2 3 1 2 0 3 2 3 1 2 2 3 1 0 1 2 1 1 2 4 0 2 0 4 4\n",
      " 1 1 1 2 4 2 2 3 3 2 0 0 1 2 4 2 3 2 3 2 1 2 2 1 3 2 4 2 2 1 4 1 3 4 4 3 0\n",
      " 1 3 0 3 1 4 3 2 1 1 3 1 4 3 1 0 1 3 2 3 0 1 4 0 2 4 3 2 3 2 0 4 3 2 0 3 4\n",
      " 0]\n",
      "Support vector machine,Tfidf word level Vectors:  0.9775280898876404\n",
      "[0 2 0 2 0 0 4 4 4 1 2 1 3 0 1 4 4 0 3 4 3 1 2 4 1 1 2 4 4 4 0 2 3 0 0 2 0\n",
      " 3 2 0 3 3 3 1 1 2 1 0 1 1 2 0 0 0 0 3 3 1 4 0 4 3 3 1 2 4 0 2 1 3 4 4 3 4\n",
      " 3 0 3 3 0 1 0 4 2 2 2 0 2 3 2 3 1 3 3 3 2 0 1 4 4 4 1 2 1 0 4 1 2 1 1 0 0\n",
      " 1 2 1 2 2 4 3 2 1 2 3 2 3 0 4 4 0 3 0 2 1 0 2 0 1 2 0 3 3 1 3 1 1 2 4 4 3\n",
      " 2 2 4 1 3 0 2 4 0 1 2 4 0 0 2 4 0 2 3 2 2 3 4 1 2 0 2 1 4 3 1 2 0 3 2 1 1\n",
      " 4 1 4 4 3 0 4 4 3 1 4 3 4 4 0 2 1 0 2 4 4 1 2 4 0 3 0 4 1 2 2 1 2 3 3 2 1\n",
      " 0 1 1 0 4 2 0 4 2 3 0 2 1 0 0 0 4 1 4 3 3 2 4 3 4 4 2 1 0 1 3 2 3 1 3 2 2\n",
      " 1 4 1 0 1 2 0 1 4 0 2 4 1 4 1 1 3 4 0 3 3 1 1 1 0 3 0 1 0 0 1 0 4 3 3 3 0\n",
      " 3 2 2 2 3 2 3 4 4 3 2 3 1 2 2 1 0 2 3 0 3 0 2 1 4 3 0 4 1 3 2 4 2 3 1 2 1\n",
      " 2 1 3 4 2 3 4 3 1 1 1 1 3 0 2 3 3 2 2 2 2 3 2 0 1 4 0 3 4 0 2 3 2 2 1 4 3\n",
      " 4 4 0 4 3 1 0 0 3 3 2 0 1 3 0 0 2 1 2 1 1 4 4 0 3 2 2 0 2 3 2 3 3 2 3 0 2\n",
      " 1 3 2 0 4 4 2 3 1 0 0 2 4 3 0 1 0 4 2 2 2 1 4 0 1 2 2 0 2 1 0 0 0 3 1 3 4\n",
      " 1]\n",
      "Support vector machine,Tfidf ngram Vectors:  0.9707865168539326\n"
     ]
    }
   ],
   "source": [
    "# support Vector machine on count vector\n",
    "accuracy = train_model(svm.SVC(),cvtrain_x,cvtest_x,cvtrain_y,cvtest_y)\n",
    "print(\"Support vector machine,count Vectors: \", accuracy)\n",
    "\n",
    "# support Vector machine on tfidf vector\n",
    "accuracy = train_model(svm.SVC(),tfidf_train_x,tfidf_test_x,tfidf_train_y,tfidf_test_y)\n",
    "print(\"Support vector machine,Tfidf word level Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "# support Vector machine on tfidf ngram vector\n",
    "accuracy = train_model(svm.SVC(),ngram_train_x,ngram_test_x,ngram_train_y,ngram_test_y)\n",
    "print(\"Support vector machine,Tfidf ngram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94ed2beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hutchins had been working as director of photography on the set of Rust.\\n\\nAmerican Cinematographer magazine had named her one of its rising stars in 2019, and she previously worked on 2020 independent superhero film Archenemy.\\n\\nArchenemy director Adam Egypt Mortimer told BBC News the fact she had died on a set was \"really unbelievable\".He said: \"Halyna was an incredible artist who was just starting a career I think people were really starting to notice.\\n\\n\"The fact that she would be killed on a set in an accident like this is unfathomable. It just seems inconceivable.\" Hutchins\\' most recent post on Instagram, from Tuesday, showed her riding horses on set. On Twitter, Alec Baldwin said \"there are no words to convey my shock and sadness regarding the tragic accident that took the life of Halyna Hutchins, a wife, mother and deeply admired colleague of ours.\"\\n\\n\"My heart is broken for her husband, their son, and all who knew and loved Halyna,\" he added.Fellow cinematographer Catherine Goldschmidt described Hutchins as \"lovely, warm, funny, charming, outgoing\", and praised her for being \"so talented\".\\n\\n\"What\\'s so tragic is she\\'s made beautiful films already but when you think about what was ahead of her, that is also so sad,\" she told BBC News.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('input.txt', 'rt') as myfile:  # Open lorem.txt for reading text\n",
    "    contents = myfile.read()              # Read the entire file to a string\n",
    "\n",
    "\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13ee7cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   output\n",
       "0       3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('input.txt', 'rt') as myfile:  # Open lorem.txt for reading text\n",
    "    contents = myfile.read()              # Read the entire file to a string\n",
    "   \n",
    "\n",
    "\n",
    "text = [contents]\n",
    "data = {'input':text}\n",
    "\n",
    "dframe = pd.DataFrame(data)\n",
    "dframe\n",
    "\n",
    "\n",
    "#utility function for prediction\n",
    "\n",
    "\n",
    "\n",
    "model = naive_bayes.MultinomialNB()\n",
    "model.fit(cvtrain_x, cvtrain_y)\n",
    "\n",
    "vec2 = count_vect.transform(dframe[\"input\"])\n",
    "\n",
    "y_prediction = model.predict(vec2)\n",
    "y_prediction\n",
    "\n",
    "arr = y_prediction\n",
    "\n",
    "li = [arr]\n",
    "pd.DataFrame(data = li, columns= [\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d52f739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x29421 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 126 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb441189",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'docClassficationModel.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca46beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer\n",
    "vec_file = 'vectorizer.pickle'\n",
    "pickle.dump(count_vect, open(vec_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401fc4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9842696629213483"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3fbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
